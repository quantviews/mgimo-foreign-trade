# Документация для `merge_processed_data.py`

Этот документ описывает работу скрипта `merge_processed_data.py`, который является финальным шагом в процессе объединения данных о внешней торговле из различных источников.

## Описание

Скрипт `merge_processed_data.py` предназначен для объединения всех обработанных наборов данных (в формате `.parquet` из папки `data_processed/`) в единую базу данных DuckDB. Он также позволяет опционально включать данные из базы Comtrade для стран, по которым отсутствуют национальные данные, и предоставляет гибкие опции фильтрации.

Скрипт включает строгую валидацию данных на этапе загрузки, что гарантирует целостность и соответствие данных ожидаемой схеме. При обнаружении ошибок валидации обработка файла прекращается с соответствующим сообщением об ошибке.

## Логика работы скрипта

Скрипт выполняет следующие шаги:

1.  **Поиск национальных данных**: Скрипт сканирует директорию `data_processed/` на наличие `.parquet` файлов (например, `ch_full.parquet`, `tr_full.parquet`).

2.  **Загрузка и валидация**: Каждый найденный `.parquet` файл загружается и проходит строгую проверку:
    *   **Валидация схемы**: Структура данных (названия колонок, типы данных) сверяется с эталонной схемой, определенной в `EXPECTED_SCHEMA`. При обнаружении несоответствий (отсутствующие колонки, неправильные типы данных, невалидные значения в `NAPR` или пустые значения в `PERIOD`) файл отклоняется с ошибкой, и обработка прекращается.
    *   **Генерация производных колонок**: Скрипт автоматически генерирует колонки `TNVED2`, `TNVED4`, `TNVED6`, `TNVED8` на основе основной колонки `TNVED`. 
        *   **Нормализация кодов TNVED**: Коды TNVED нормализуются для обеспечения единообразия:
            *   Сначала удаляются все ведущие нули (например, `"0000870421"` → `"870421"`)
            *   Затем код дополняется нулями **справа** до 10 знаков (например, `"870421"` → `"8704210000"`)
            *   Это гарантирует, что коды вида `"870421"` (8 знаков) и `"87042100"` (10 знаков) будут приведены к единому формату `"8704210000"` (10 знаков)
        *   После нормализации производные колонки вычисляются как срезы строки соответствующей длины (`TNVED2` = первые 2 знака, `TNVED4` = первые 4 знака и т.д.)

3.  **Интеграция данных Comtrade (опционально)**:
    *   Если при запуске указан флаг `--include-comtrade`, скрипт подключается к базе данных `db/comtrade.db`.
    *   **Исключение дубликатов**: Чтобы избежать дублирования, скрипт сначала определяет, какие страны уже присутствуют в национальных данных (например, `CN`, `TR`, `IN`), и **исключает** их из запроса к Comtrade. После загрузки данных Comtrade выполняется дополнительная проверка для удаления любых записей, которые могли проскользнуть через фильтр.
    *   **Трансформация данных**: Данные из Comtrade преобразуются в соответствии с моделью данных проекта:
        *   Числовые M49-коды стран (`reporterCode`) переводятся в двухбуквенные ISO-коды (`STRANA`) с помощью `metadata/comtrate-partnerAreas.json`.
        *   Коды единиц измерения (`qtyUnitCode`, `altQtyUnitCode`) переводятся в их текстовые аббревиатуры (`EDIZM`) с помощью `metadata/comtradte-QuantityUnits.json`.
        *   **Выбор дополнительной единицы**: Скрипт анализирует основную и альтернативную единицы измерения. Так как вес в кг уже есть в колонке `NETTO`, для колонки `KOL` (доп. количество) выбирается не-весовая единица (например, штуки, литры).
        *   Направление торговли (`flowCode`) 'M' и 'X' преобразуется в 'ИМ' и 'ЭК'.
        *   Генерируются производные колонки `TNVED2`, `TNVED4`, `TNVED6`, `TNVED8` на основе `TNVED`. Коды TNVED нормализуются так же, как и для национальных данных: удаляются ведущие нули, затем код дополняется нулями справа до 10 знаков.

4.  **Фильтрация данных**:
    *   **По году**: Если указан аргумент `--start-year`, все наборы данных (и национальные, и Comtrade) фильтруются, чтобы оставить только записи, начиная с указанного года.
    *   **По странам**: Если указан аргумент `--exclude-countries`, все записи, относящиеся к перечисленным странам, удаляются из итогового набора данных.

5.  **Объединение и очистка**:
    *   Все подготовленные наборы данных (национальные и Comtrade) объединяются в один DataFrame.
    *   В каждую запись добавляется колонка `SOURCE`, указывающая на источник (`national` или `comtrade`).
    *   Удаляются все строки, в которых отсутствует значение в колонке `NAPR` (направление торговли).

6.  **Стандартизация единиц измерения (EDIZM)**:
    *   Скрипт использует `metadata/edizm.csv` для приведения всех значений в колонке `EDIZM` к единому стандарту (например, 'kg', 'Kilogram', 'КИЛОГРАММ' будут приведены к одному общему виду).
    *   На основе стандартизированного значения заполняется колонка `EDIZM_ISO` соответствующим ISO-кодом.

7.  **Очистка дублирующих данных о весе**:
    *   **Обработка килограммов**: Чтобы избежать дублирования данных с колонкой `NETTO`, скрипт находит все строки, где дополнительная единица измерения (`EDIZM_ISO`) соответствует ISO-коду килограмма (166). В этих строках значения `KOL`, `EDIZM` и `EDIZM_ISO` устанавливаются в `NULL` (присваиваются по отдельности для каждой колонки).
    *   **Обработка тонн**: Скрипт применяет интеллектуальную логику для тонн (ISO-код 168):
        *   Если в строке указано `KOL` в тоннах, а колонка `NETTO` **пустая или равна нулю**, то значение из `KOL` конвертируется в кг (`KOL` * 1000) и записывается в `NETTO`. После этого `KOL`, `EDIZM` и `EDIZM_ISO` устанавливаются в `NULL` (присваиваются по отдельности для каждой колонки).
        *   Если же в строке `NETTO` **уже заполнено**, то `KOL` в тоннах считается дублирующей информацией, и значения `KOL`, `EDIZM` и `EDIZM_ISO` устанавливаются в `NULL` (присваиваются по отдельности для каждой колонки).

8.  **Обнаружение и обработка выбросов (опционально)**:
    *   По умолчанию скрипт выполняет обнаружение выбросов в колонке `KOL` (количество в дополнительной единице измерения) и заменяет их на `NULL`. Этот процесс можно отключить с помощью флага `--skip-outlier-detection` или изменить поведение с помощью флага `--keep-outliers`.
    *   **Методология обнаружения**: Выбросы определяются внутри каждого временного ряда, группируя данные по комбинации `STRANA` (страна), `TNVED` (код товара) и `NAPR` (направление торговли). Используются три независимых метода обнаружения:
        *   **Метод 1**: Выбросы в абсолютных значениях `KOL` — значения, которые отклоняются более чем на 6 стандартных отклонений от среднего значения ряда и превышают порог 1,000,000.
        *   **Метод 2**: Выбросы в отношении `KOL/STOIM` — значения, где отношение количества к стоимости отклоняется более чем на 6 стандартных отклонений от среднего отношения в ряду, и при этом `KOL` превышает порог 1,000,000.
        *   **Метод 3**: Выбросы в отношении `KOL/NETTO` — значения, где отношение количества к весу отклоняется более чем на 6 стандартных отклонений от среднего отношения в ряду, и при этом `KOL` превышает порог 1,000,000.
    *   **Критерий выброса**: Временной ряд считается содержащим выбросы только если **все три метода** обнаружили хотя бы один выброс в этом ряду. Это обеспечивает высокую точность обнаружения и снижает количество ложных срабатываний.
    *   **Обработка выбросов**: По умолчанию все обнаруженные выбросы заменяются на `NULL` (NaN) в колонке `KOL`. Это позволяет сохранить остальные данные ряда, удалив только аномальные значения. Поведение можно изменить с помощью аргументов командной строки:
        *   Без флагов: выбросы обнаруживаются и заменяются на `NULL` (поведение по умолчанию).
        *   `--keep-outliers`: выбросы обнаруживаются, но остаются в данных без изменений.
        *   `--skip-outlier-detection`: процесс обнаружения выбросов полностью пропускается.
    *   **Создание отчетов**: При обнаружении выбросов скрипт автоматически создает детальные отчеты в папке `reports/`:
        *   `outliers_detailed_YYYYMMDD_HHMMSS.csv` — детальный отчет по каждому обнаруженному выбросу с указанием даты, значений, z-score и методов обнаружения.
        *   `outliers_summary_YYYYMMDD_HHMMSS.csv` — сводная таблица по временным рядам с количеством выбросов по каждому методу.
        *   `outliers_report_YYYYMMDD_HHMMSS.json` — метаданные отчета с параметрами обнаружения, статистикой и информацией о замене.
    *   **Параметры обнаружения**: Используются параметры из R-скрипта `outliers_detection.Rmd`:
        *   `nsd = 6.0` — количество стандартных отклонений (6 сигм)
        *   `tv = 1,000,000` — пороговое значение для `KOL`

9.  **Создание справочных таблиц**:
    *   **Нормализованная структура**: скрипт создает отдельные справочные таблицы для нормализации структуры базы данных:
        *   `country_reference` — таблица с соответствием ISO-кодов стран (`STRANA`) и их названий (`STRANA_NAME`) из `metadata/STRANA.csv`
        *   `tnved_reference` — таблица с кодами TNVED (`TNVED_CODE`), уровнями агрегации (`TNVED_LEVEL`: 2, 4, 6, 8, 10) и названиями (`TNVED_NAME`) из `metadata/tnved.csv`
    *   **Представление для удобства**: Создается представление `unified_trade_data_enriched`, которое автоматически объединяет основную таблицу со справочными таблицами, предоставляя все названия в удобном формате. Это позволяет использовать либо нормализованную структуру (для оптимизации), либо представление (для удобства в Superset).
    *   **Индексы**: На справочных таблицах создаются индексы для ускорения операций JOIN.

10. **Сохранение результата**:
    *   Финальный, объединенный и очищенный DataFrame сохраняется в базу данных `db/unified_trade_data.duckdb`.
    *   Если база данных уже существует, она будет **полностью удалена** перед созданием новой для обеспечения чистоты данных.
    *   Данные сохраняются порциями (чанками) по 100,000 строк для экономии памяти.
    *   После сохранения основной таблицы создаются справочные таблицы (`country_reference`, `tnved_reference`) и представление `unified_trade_data_enriched`.
    *   После сохранения выполняется проверка количества записей для подтверждения корректности сохранения.

### Структура базы данных

База данных `unified_trade_data.duckdb` содержит следующие объекты:

*   **`unified_trade_data`** — основная таблица с данными о внешней торговле (без названий стран и TNVED для экономии места).
*   **`country_reference`** — справочная таблица с названиями стран (колонки: `STRANA`, `STRANA_NAME`).
*   **`tnved_reference`** — справочная таблица с названиями кодов TNVED (колонки: `TNVED_CODE`, `TNVED_LEVEL`, `TNVED_NAME`).
*   **`unified_trade_data_enriched`** — представление (VIEW), которое объединяет основную таблицу со справочными таблицами, добавляя все названия и ранжирование по периодам. Это удобно для использования в Superset, так как не требует ручного создания JOIN'ов.
    *   **Дополнительные колонки в представлении:**
        *   `COUNTRY_NAME` — название страны из справочника
        *   `TNVED2_NAME`, `TNVED4_NAME`, `TNVED6_NAME`, `TNVED8_NAME` — названия кодов TNVED соответствующих уровней
        *   `TNVED_NAME` — название полного кода TNVED (10 знаков), если доступно, иначе название уровня 8
        *   `period_rank` — порядковый номер записи по периоду для каждой страны (1 = самый последний месяц, 2 = предпоследний и т.д.). Это позволяет легко фильтровать последние N месяцев для каждой страны независимо от того, когда обновляются данные по разным странам.

**Рекомендации по использованию:**
*   Для аналитики в Superset используйте представление `unified_trade_data_enriched` — оно содержит все необходимые данные с названиями.
*   Для получения последних 12 месяцев для каждой страны используйте фильтр `WHERE period_rank <= 12`.
*   Для оптимизированных запросов, где названия не нужны, используйте таблицу `unified_trade_data` напрямую.
*   При необходимости можно создавать собственные JOIN'ы между `unified_trade_data` и справочными таблицами для более гибкой настройки.

11. **Вывод статистики**: В процессе работы скрипт выводит в консоль подробную статистику о результатах объединения, включая:
    *   Общее количество строк
    *   Количество уникальных стран
    *   Диапазон дат (минимальная и максимальная даты)
    *   Количество записей по каждому источнику (`national` или `comtrade`)
    *   Количество записей по каждой стране и источнику
    *   Распределение основных единиц измерения (`EDIZM`) по странам (топ-5 для каждой страны)
    *   Статистика обнаружения выбросов (если не использован флаг `--skip-outlier-detection`):
        *   Количество временных рядов с выбросами
        *   Количество выбросов по каждому методу обнаружения
        *   Топ-10 временных рядов с наибольшим количеством выбросов
        *   Количество замененных значений (если выбросы были заменены)

## Как запустить скрипт

Скрипт запускается из командной строки и поддерживает несколько аргументов для управления процессом объединения.

### Аргументы командной строки

*   `--include-comtrade` (опциональный): Флаг, указывающий на необходимость включения данных из Comtrade.
*   `--start-year <год>` (опциональный): Целое число. Если указано, в итоговый набор попадут только данные, начиная с этого года.
*   `--exclude-countries <ISO-код1> <ISO-код2> ...` (опциональный): Список двухбуквенных ISO-кодов стран, которые нужно исключить из финального набора данных.
*   `--keep-outliers` (опциональный): Флаг, указывающий на то, что выбросы должны быть обнаружены, но оставлены в данных без изменений (по умолчанию выбросы заменяются на `NULL`).
*   `--skip-outlier-detection` (опциональный): Флаг, указывающий на то, что процесс обнаружения и обработки выбросов должен быть полностью пропущен. Это ускоряет выполнение скрипта, если обнаружение выбросов не требуется.

### Примеры использования

**1. Простое объединение только национальных данных:**
```bash
python src/merge_processed_data.py
```

**2. Объединение национальных данных с добавлением Comtrade:**
```bash
python src/merge_processed_data.py --include-comtrade
```

**3. Объединение всех данных, начиная с 2019 года:**
```bash
python src/merge_processed_data.py --include-comtrade --start-year 2019
```

**4. Объединение всех данных, но с исключением Индии:**
```bash
python src/merge_processed_data.py --include-comtrade --exclude-countries IN
```

**5. Объединение данных за все года, исключая Китай и Турцию:**
```bash
python src/merge_processed_data.py --include-comtrade --exclude-countries CN TR
```

**6. Объединение данных с обнаружением выбросов, но без их замены:**
```bash
python src/merge_processed_data.py --include-comtrade --keep-outliers
```

**7. Объединение данных без обнаружения выбросов (ускоренный режим):**
```bash
python src/merge_processed_data.py --include-comtrade --skip-outlier-detection
```

## Технические детали и улучшения

### Валидация данных

Скрипт использует строгую валидацию на этапе загрузки данных:

*   **Проверка обязательных колонок**: Все колонки из `EXPECTED_SCHEMA` должны присутствовать в файле.
*   **Проверка типов данных**: Типы данных должны точно соответствовать ожидаемым. При несоответствии файл отклоняется.
*   **Валидация значений**: 
    *   Колонка `NAPR` может содержать только значения 'ИМ' или 'ЭК'.
    *   Колонка `PERIOD` не может содержать пустые значения.
*   **Автоматическое преобразование**: Колонка `PERIOD` автоматически преобразуется в формат `datetime64[ns]` при необходимости.

### Оптимизация производительности

*   **Эффективная генерация производных колонок**: Колонки `TNVED2`, `TNVED4`, `TNVED6`, `TNVED8` генерируются напрямую без предварительных проверок, что повышает производительность.
*   **Правильная нормализация кодов TNVED**: Коды нормализуются путем удаления ведущих нулей и дополнения справа, что обеспечивает корректное сопоставление с справочником `tnved.csv` и правильную работу JOIN'ов в представлении `unified_trade_data_enriched`.
*   **Оптимизированная обработка маппингов**: Использование `itertuples()` вместо `iterrows()` для более быстрой обработки справочников.
*   **Чанковая запись в базу данных**: Данные записываются порциями по 100,000 строк для экономии памяти при работе с большими объемами данных.
*   **Нормализованная структура базы данных**: Справочные данные (названия стран и TNVED) хранятся в отдельных таблицах, что:
    *   Уменьшает размер основной таблицы (названия не дублируются в каждой строке)
    *   Упрощает обновление справочников без изменения основной таблицы
    *   Позволяет DuckDB эффективно использовать индексы при JOIN'ах
    *   Оптимизирует работу с Superset, который эффективно обрабатывает JOIN'ы
*   **Ранжирование по периодам**: Колонка `period_rank` в представлении `unified_trade_data_enriched` позволяет эффективно фильтровать последние N месяцев для каждой страны без необходимости сложных подзапросов.

### Безопасность

*   **Проверка типов в SQL запросах**: При формировании SQL запросов к базе Comtrade выполняется явная проверка типов данных (M49 коды должны быть целыми числами) перед форматированием запроса, что предотвращает потенциальные проблемы с безопасностью.
*   **Валидация входных данных**: Все входные параметры проверяются на корректность типов перед использованием.

### Обнаружение выбросов

Скрипт реализует продвинутую систему обнаружения выбросов в колонке `KOL` на основе методологии из R-скрипта `outliers_detection.Rmd`:

*   **Группировка по временным рядам**: Выбросы определяются внутри каждого временного ряда (комбинация `STRANA`, `TNVED`, `NAPR`), что позволяет учитывать специфику каждого товарного потока.
*   **Три независимых метода**: Использование трех методов обнаружения обеспечивает высокую точность и снижает количество ложных срабатываний.
*   **Строгий критерий**: Временной ряд считается содержащим выбросы только если все три метода обнаружили хотя бы один выброс.
*   **Z-score анализ**: Используется стандартизированное отклонение (z-score) для определения аномальных значений относительно среднего и стандартного отклонения ряда.
*   **Пороговые значения**: Параметры обнаружения (6 стандартных отклонений, порог 1,000,000) были подобраны опытным путем и соответствуют параметрам из R-скрипта.
*   **Автоматическое создание отчетов**: Все обнаруженные выбросы документируются в детальных отчетах, сохраняемых в папке `reports/` с временными метками для отслеживания истории обработки.

### Обработка ошибок

При обнаружении ошибок валидации скрипт:
*   Логирует подробное сообщение об ошибке с указанием файла и типа проблемы.
*   Прекращает обработку проблемного файла и переходит к следующему.
*   Продолжает работу с остальными файлами, что позволяет получить максимально возможный результат даже при наличии проблемных данных.
