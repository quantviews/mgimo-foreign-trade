# Документация и логика работы `china-collector.py` и `china_processor.py`

## 1. Назначение скриптов

### `china-collector.py`
Скрипт предназначен для полуавтоматического сбора **сырых данных** по внешней торговле с сайта Таможенной службы Китая ([http://stats.customs.gov.cn/indexEn](http://stats.customs.gov.cn/indexEn)) о внешней торговле с РФ. 
Он автоматизирует открытие браузера и подготовку к скачиванию, оставляя пользователю ручное заполнение формы и прохождение проверки CAPTCHA.

### `china_processor.py`
Скрипт предназначен для **обработки, проверки качества и объединения** всех сырых файлов, собранных `china-collector.py`. Он сканирует директорию с сырыми данными, приводит каждый файл к стандартному формату, выполняет проверку на полноту данных и сохраняет итоговый набор в формате Parquet.

## 2. Логика работы (Workflow)

### Шаг 1: Сбор сырых данных с `china-collector.py`

1.  **Запуск скрипта:** Пользователь запускает скрипт из командной строки, указывая год, месяц и направление торгового потока (`ИМ` для импорта, `ЭК` для экспорта).
    ```bash
    python src/collectors/china-collector.py 2025 7 ИМ --partner 344
    ```
2.  **Открытие браузера:** Скрипт автоматически открывает браузер Google Chrome и переходит на сайт таможенной статистики КНР.
3.  **Действия пользователя:** В открывшемся окне браузера пользователь должен вручную:
    *   Заполнить форму, выбрав необходимые параметры (торговый поток, период, страну-партнера (РФ = 344), валюту (US Dollar) и т.д.).
    *   Нажать кнопку "Enquiry".
    *   Пройти проверку CAPTCHA (слайдер).
    *   На странице с результатами нажать кнопку "Download", чтобы скачать файл `downloadData.csv`.
4.  **Завершение сбора:** После того как файл `downloadData.csv` полностью скачан, пользователь возвращается в окно терминала и нажимает клавишу `Enter`.
5.  **Автоматическое сохранение:** Скрипт находит скачанный файл, перемещает его в `data_raw/china/` и переименовывает в `data{год}{месяц}.csv` (например, `data202507.csv`). После этого он проводит проверку качества сырых данных.

Этот шаг можно повторять для сбора данных за разные месяцы и годы.

### Шаг 2: Обработка всех данных с `china_processor.py`

1.  **Запуск скрипта:** Пользователь запускает процессор без аргументов.
    ```bash
    python src/collectors/china_processor.py
    ```
2.  **Поиск файлов:** Скрипт автоматически ищет все файлы `data*.csv` в директориях `data_raw/china/IMPORT` и `data_raw/china/EXPORT`.
3.  **Очистка и обработка:** Каждый найденный файл проходит стандартизацию: приводятся к единому формату колонки, очищаются числовые поля (`NETTO`, `KOL`) от лишних символов и форматируются коды ТН ВЭД.
4.  **Объединение и проверка:** Все обработанные данные объединяются в один набор. После этого скрипт выполняет **проверку на наличие пропусков** — отсутствующих месяцев во временном ряду. Если найдены пропуски, выводится предупреждение.
5.  **Сохранение результата:** Итоговый набор данных сохраняется в файл `data_processed/ch_full.parquet`. Если такой файл уже существует, он будет **удален и создан заново**.

## 3. Использование

### Требования
-   Установленный Python и `pip`.
-   Активированное окружение `conda` (`mgimo_trade`).
-   Установленный браузер Google Chrome.

### Установка зависимостей
```bash
pip install pandas selenium undetected-chromedriver pyarrow
```

### Полный рабочий процесс

1.  **Сбор сырых данных (можно выполнять многократно):**
    ```bash
    python src/collectors/china-collector.py 2025 7 ИМ --partner 344
    python src/collectors/china-collector.py 2025 8 ИМ --partner 344
    ```

2.  **Обработка и объединение всех данных в один файл:**
    ```bash
    python src/collectors/china_processor.py
    ```

## 4. Связанные файлы

-   **`china-collector.py`**: Скрипт для сбора сырых данных.
-   **`china_processor.py`**: Скрипт для обработки и объединения сырых данных.
-   **`data_raw/china/`**: Директория для хранения сырых данных.
-   **`data_processed/ch_full.parquet`**: Финальный, обработанный файл с данными.
