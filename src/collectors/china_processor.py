import pandas as pd
from pathlib import Path
import os

def process_and_merge_china_data(raw_data_dir: Path, output_file: Path):
    """
    Scans for raw china data files, processes them according to the logic
    from ProcessData.ipynb, merges them into a single DataFrame, and saves
    as a Parquet file.
    """
    
    # 1. Find all raw data files generated by the collector
    print("Searching for raw data files...")
    import_dir = raw_data_dir / 'IMPORT'
    export_dir = raw_data_dir / 'EXPORT'
    
    all_files = list(import_dir.glob('data*.csv')) + list(export_dir.glob('data*.csv'))

    if not all_files:
        print("No raw data files found to process in 'data_raw/china/'.")
        return

    print(f"Found {len(all_files)} files to process.")
    
    # 2. Process each file and collect DataFrames
    processed_dfs = []
    for file_path in all_files:
        try:
            df = pd.read_csv(file_path)

            # Apply transformations based on refresh_ch()
            if 'PERIOD' in df.columns:
                df['PERIOD'] = df['PERIOD'] + '-01'
            
            if 'STRANA' in df.columns:
                df['STRANA'] = 'CN'

            # Rename columns to match the old notebook's standard
            df = df.rename(columns={
                'Supplementary Quantity': 'KOL',
                'Supplementary Unit': 'EDIZM'
            })

            # Ensure TNVED codes are zero-padded strings
            tnved_cols = {'TNVED': 8, 'TNVED2': 2, 'TNVED4': 4, 'TNVED6': 6}
            for col, length in tnved_cols.items():
                if col in df.columns:
                    df[col] = df[col].astype(str).str.zfill(length)

            # Reorder columns to a standard format
            standard_columns = [
                'NAPR', 'PERIOD', 'STRANA', 'TNVED', 'EDIZM', 'STOIM', 
                'NETTO', 'KOL', 'TNVED4', 'TNVED6', 'TNVED2'
            ]
            
            existing_cols = [col for col in standard_columns if col in df.columns]
            df = df.reindex(columns=existing_cols)
            
            processed_dfs.append(df)
            print(f"  - Processed {file_path.name}")
        except Exception as e:
            print(f"  - FAILED to process {file_path.name}: {e}")

    if not processed_dfs:
        print("No data was successfully processed.")
        return

    # 3. Concatenate all DataFrames
    print("Merging all processed files...")
    final_df = pd.concat(processed_dfs, ignore_index=True)

    # 4. Clean data before saving
    # The NETTO column can contain non-numeric characters (e.g., tabs)
    # Convert to string, strip whitespace, and then convert to numeric, coercing errors.
    if 'NETTO' in final_df.columns:
        print("Cleaning 'NETTO' column...")
        final_df['NETTO'] = pd.to_numeric(
            final_df['NETTO'].astype(str).str.strip(), 
            errors='coerce'
        )

    if 'KOL' in final_df.columns:
        print("Cleaning 'KOL' column...")
        final_df['KOL'] = pd.to_numeric(
            final_df['KOL'].astype(str).str.strip(),
            errors='coerce'
        )

    # 5. Check for missing months in the time series
    print("Checking for gaps in the time series...")
    if 'PERIOD' in final_df.columns:
        # Ensure PERIOD is in datetime format for comparison
        final_df['PERIOD'] = pd.to_datetime(final_df['PERIOD'])
        
        # Get the unique months from the data
        actual_months = set(final_df['PERIOD'].dt.to_period('M'))
        
        # Create a complete date range from min to max date
        start_date = final_df['PERIOD'].min()
        end_date = final_df['PERIOD'].max()
        expected_months = set(pd.period_range(start=start_date, end=end_date, freq='M'))
        
        missing_months = sorted(list(expected_months - actual_months))
        
        if missing_months:
            print("\n[WARNING] Missing data for the following months:")
            for month in missing_months:
                print(f"  - {month}")
        else:
            print("  [PASS] No missing months found in the time series.")

    # 6. Delete existing parquet file if it exists
    if output_file.exists():
        print(f"Deleting existing file: {output_file}")
        os.remove(output_file)

    # 7. Save final DataFrame as Parquet
    output_file.parent.mkdir(parents=True, exist_ok=True)
    
    print(f"\nSaving merged data to {output_file}...")
    try:
        final_df.to_parquet(output_file, index=False)
        print("Processing complete.")
    except ImportError:
        print("\n[Error] 'pyarrow' is required to save to Parquet format.")
        print("Please install it using: pip install pyarrow")


def main():
    """
    Main function to run the China data processing script.
    """
    project_root = Path(__file__).resolve().parent.parent.parent
    raw_data_dir = project_root / 'data_raw' / 'china'
    output_file = project_root / 'data_processed' / 'ch_full.parquet'
    
    process_and_merge_china_data(raw_data_dir, output_file)

if __name__ == "__main__":
    main()
